{"cells":[{"cell_type":"markdown","id":"0ebfd27b-7e8a-4dca-b736-1224ecc56edd","metadata":{"id":"0ebfd27b-7e8a-4dca-b736-1224ecc56edd"},"source":["# In this snippet we will learn about Remote Sensing (RS) image captioning technique using encoder-decoder based method"]},{"cell_type":"markdown","id":"07baaff5-818c-4a12-ba85-32b87473d1c0","metadata":{"id":"07baaff5-818c-4a12-ba85-32b87473d1c0"},"source":["<div style=\"text-align: justify\">\n","In image captioning, a machine automatically generates descriptive text to accompany an image. This process typically utilizes a combination of computer vision and natural language processing techniques. One commonly used model for this task is the encoder-decoder model. Initially, the image undergoes analysis using convolutional neural networks (CNNs) to extract relevant features and comprehend its content. These features are then inputted into recurrent neural networks (RNNs) or transformer-based models to produce the caption. RNNs are favored for their effectiveness in handling sequential data. During the training phase, the model learns to associate visual features with corresponding words or phrases by leveraging large datasets containing images paired with human-generated captions. Image captioning finds applications across various domains, including assistive technology, content accessibility, and enhancing social media content. One typical example is depicted below:\n","</div>\n","\n","![Caption](images/Captions.png)\n","\n","<div style=\"text-align: justify\">\n","There are two kinds of captioning techniques:<br>\n","<b>Natural Image Captioning</b><br>\n","<b>Remote Sensing Image Captioning</b>\n","</div>"]},{"cell_type":"markdown","id":"172421a7-f813-4c71-817e-386c186c4b0f","metadata":{"id":"172421a7-f813-4c71-817e-386c186c4b0f"},"source":["## Encoder-Decoder model\n","\n","<div style=\"text-align: justify\">\n","In remote sensing image captioning, an encoder-decoder model is utilized to generate textual descriptions for satellite or aerial images automatically. The encoder-decoder architecture comprises two main components: the encoder, responsible for understanding the visual content of the image, and the decoder, which generates the corresponding textual description.<br>\n","The encoder, often based on convolutional neural networks (CNNs), processes the input image to extract high-level features. In the context of remote sensing, these features may represent various aspects such as land cover types, terrain features, or man-made structures. The CNN layers capture spatial information hierarchically, transforming the raw pixel values into a compact and semantically rich representation. Transfer learning techniques, where pre-trained CNN models on large-scale datasets like ImageNet, are commonly employed to leverage their learned feature representations and adapt them to the remote sensing domain with relatively small annotated datasets.<br>\n","The decoder, typically implemented as a recurrent neural network (RNN) or transformer architecture, takes the encoded features from the CNN and generates the textual description. RNN-based decoders, such as Long Short-Term Memory (LSTM) networks, produce captions sequentially, generating one word at a time while incorporating context from previously generated words. Transformers, on the other hand, process the encoded features in parallel, attending to different parts of the image features to generate the caption.<br>\n","During training, the model learns to associate visual features with textual descriptions by minimizing a loss function that measures the dissimilarity between the generated caption and ground truth annotations. Additionally, attention mechanisms are often incorporated to allow the model to focus on relevant regions of the image while generating each word in the caption. Evaluation metrics like BLEU (Bilingual Evaluation Understudy) and METEOR (Metric for Evaluation of Translation with Explicit Ordering) are commonly used to assess the quality of generated captions. Overall, the encoder-decoder model in remote sensing image captioning facilitates automated analysis and interpretation of large-scale satellite or aerial imagery, enabling applications in environmental monitoring, urban planning, and disaster management.<br>\n","Architecture of Encoder-Decoder model is depicted below:\n","</div>\n","\n","![Architure](images/Architecture.png)"]},{"cell_type":"markdown","id":"ea61c1d0-3dce-421a-91bd-c36076abc02a","metadata":{"id":"ea61c1d0-3dce-421a-91bd-c36076abc02a"},"source":["## Lets start coding"]},{"cell_type":"markdown","id":"e6e23840-58f8-41ec-a52e-bb040c864dea","metadata":{"id":"e6e23840-58f8-41ec-a52e-bb040c864dea"},"source":["### Impoer libreries"]},{"cell_type":"code","execution_count":2,"id":"70c743de-d3fe-4d51-b298-b4023e7ea483","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"70c743de-d3fe-4d51-b298-b4023e7ea483","executionInfo":{"status":"error","timestamp":1715329623615,"user_tz":-330,"elapsed":625,"user":{"displayName":"Kumar Munish","userId":"16446387180300821596"}},"outputId":"592a5fc2-fafb-49d7-aec2-2fd7d2b82653"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'evaluate'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-59be64228f6e>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mredirect_stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import torch\n","import torch.nn as nn\n","from torchvision.models import resnet152, ResNet152_Weights\n","from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n","import torch.optim as optim\n","from tqdm import tqdm\n","import os\n","import shutil\n","import pickle as pkl\n","import json\n","import numpy as np\n","from PIL import Image\n","import random\n","import math\n","import time\n","import torch.nn.functional as F\n","from contextlib import redirect_stdout\n","import evaluate\n","from scipy.stats import gmean\n","import warnings\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","convert = Compose([ToTensor()])"]},{"cell_type":"markdown","id":"daf2fe30-e3de-4808-95e3-a7c948af4250","metadata":{"id":"daf2fe30-e3de-4808-95e3-a7c948af4250"},"source":["### Set Paths"]},{"cell_type":"code","execution_count":null,"id":"5281867b-faeb-4eaf-a0da-0f315a812d6b","metadata":{"id":"5281867b-faeb-4eaf-a0da-0f315a812d6b"},"outputs":[],"source":["dataset = 'SYDNEY'\n","image_path = f'/tf/DRDO/datasets/{dataset}/images'\n","sent_path = f'/tf/DRDO/datasets/{dataset}/sentences/Dataset_Modified.json'\n","file_path = f'/tf/DRDO/content/{dataset}/files'\n","model_path = f'/tf/DRDO/content/{dataset}/models'\n","pred_path = f'/tf/DRDO/content/{dataset}/preds'"]},{"cell_type":"code","execution_count":null,"id":"e5280682-8d3f-41f2-886e-c090d819c57d","metadata":{"id":"e5280682-8d3f-41f2-886e-c090d819c57d"},"outputs":[],"source":["# %%script false --no-raise-error\n","\n","def create_dir(folder_name):\n","    if os.path.exists(folder_name):\n","        shutil.rmtree(folder_name)\n","    os.makedirs(folder_name)\n","\n","create_dir(file_path)\n","create_dir(model_path)"]},{"cell_type":"markdown","id":"b0fc2866-04ab-4118-9cf5-c570339c18ad","metadata":{"id":"b0fc2866-04ab-4118-9cf5-c570339c18ad"},"source":["### Define Image Encoder"]},{"cell_type":"code","execution_count":null,"id":"7cfbcf5a-5bce-4196-8d0b-ba4b64256ad1","metadata":{"id":"7cfbcf5a-5bce-4196-8d0b-ba4b64256ad1"},"outputs":[],"source":["class encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = resnet152(weights=ResNet152_Weights.DEFAULT)\n","\n","        # Freeze all layers\n","        for param in self.model.parameters():\n","            param.requires_grad = False\n","\n","        # Modify the last fully connected layer to match the desired output size\n","        self.model.fc = nn.Identity()\n","\n","    def forward(self, x):\n","        return self.model(x)"]},{"cell_type":"markdown","id":"9a29faeb-c16e-461b-a0df-b36f7b341d88","metadata":{"id":"9a29faeb-c16e-461b-a0df-b36f7b341d88"},"source":["### Define Captioning Model"]},{"cell_type":"code","execution_count":null,"id":"56c54e0e-05d3-4fd3-a40a-45526c320127","metadata":{"id":"56c54e0e-05d3-4fd3-a40a-45526c320127"},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self,img_feat, vocab_size, word_feat_dim, embedding_vector=None, trainable=False):\n","        super().__init__()\n","        self.random_seed()\n","\n","        #From Image\n","        self.img = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(img_feat,256),\n","            nn.GELU()\n","        )\n","\n","        #From Sequence\n","        self.seq = nn.Sequential(\n","            nn.Embedding(vocab_size,word_feat_dim),\n","            nn.Dropout(0.5),\n","            nn.LSTM(word_feat_dim,256,batch_first=True)\n","        )\n","\n","        if embedding_vector:\n","            if isinstance(embedding_vector,str):\n","                with open(embedding_vector,'rb') as f:\n","                    embedding_vector = pkl.load(f)\n","            embedding_vector = torch.tensor(embedding_vector,dtype=torch.float32)\n","            self.seq[0].weight = nn.Parameter(embedding_vector, requires_grad=trainable)\n","        # Attention Layer\n","#         self.attention = HardAttention()\n","\n","        #Rest Part\n","\n","        self.rest = nn.Sequential(\n","            nn.Linear(256, 256),\n","            nn.GELU(),\n","            nn.Linear(256, vocab_size),\n","#             nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, image_features, sequence_input):\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        image_features = image_features.to(device)\n","        sequence_input = sequence_input.to(device)\n","        im_output = self.img(image_features)\n","        seq_output, _ = self.seq(sequence_input.to(torch.long))\n","        seq_output = seq_output[:, -1, :]\n","        added_output = im_output + seq_output\n","        final_output = self.rest(added_output)\n","        return final_output\n","\n","    def random_seed(self):\n","        seed = 7777\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed)\n","\n","\n","class GRUModel(nn.Module):\n","    def __init__(self,img_feat, vocab_size, word_feat_dim, embedding_vector=None, trainable=False):\n","        super().__init__()\n","        self.random_seed()\n","\n","        #From Image\n","        self.img = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(img_feat,256),\n","            nn.GELU()\n","        )\n","\n","        #From Sequence\n","        self.seq = nn.Sequential(\n","            nn.Embedding(vocab_size,word_feat_dim),\n","            nn.Dropout(0.5),\n","            nn.GRU(word_feat_dim,256,batch_first=True)\n","        )\n","\n","        if embedding_vector:\n","            if isinstance(embedding_vector,str):\n","                with open(embedding_vector,'rb') as f:\n","                    embedding_vector = pkl.load(f)\n","            embedding_vector = torch.tensor(embedding_vector,dtype=torch.float32)\n","            self.seq[0].weight = nn.Parameter(embedding_vector, requires_grad=trainable)\n","        # Attention Layer\n","#         self.attention = HardAttention()\n","\n","        #Rest Part\n","\n","        self.rest = nn.Sequential(\n","            nn.Linear(256, 256),\n","            nn.GELU(),\n","            nn.Linear(256, vocab_size),\n","#             nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, image_features, sequence_input):\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        image_features = image_features.to(device)\n","        sequence_input = sequence_input.to(device)\n","        im_output = self.img(image_features)\n","        seq_output, _ = self.seq(sequence_input.to(torch.long))\n","        seq_output = seq_output[:, -1, :]\n","        added_output = im_output + seq_output\n","        final_output = self.rest(added_output)\n","        return final_output\n","\n","    def random_seed(self):\n","        seed = 7777\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed)"]},{"cell_type":"markdown","id":"9fa729e9-e6db-409b-901b-9e76021da5e5","metadata":{"id":"9fa729e9-e6db-409b-901b-9e76021da5e5"},"source":["### Define Captioning Model"]},{"cell_type":"code","execution_count":null,"id":"3e6b757e-36aa-464d-9738-87bf25c6c738","metadata":{"id":"3e6b757e-36aa-464d-9738-87bf25c6c738"},"outputs":[],"source":["class TrainModel(nn.Module):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.model = model\n","    def __call__(self, dataloader,batch_size=64, epochs=64, patience=None, savepath=None):\n","        self.random_seed()\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n","        val_acc = []\n","        # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        # self.model.to(device)\n","        for epoch in range(epochs):\n","            train_loss = 0\n","            correct = 0\n","            total = 0\n","            for (image,seq), labels in tqdm(dataloader(mode='train',batch_size=batch_size), desc=f'Epoch {epoch + 1}/{epochs}: Training'):\n","\n","                self.model.train()\n","                optimizer.zero_grad()\n","                outputs = self.model(image.to(device),seq.to(device))\n","                loss = criterion(outputs.to(device), labels.to(device))\n","                loss.backward()\n","                optimizer.step()\n","                train_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","#                 print(predicted.shape)\n","                total += labels.size(0)\n","                correct += predicted.eq(labels.to(device)).sum().item()\n","            train_accuracy = 100 * correct / total\n","            train_loss = train_loss / (total / batch_size)\n","            self.model.eval()\n","            val_loss = 0\n","            correct = 0\n","            total = 0\n","            with torch.no_grad():\n","                for (image,seq), labels in tqdm(dataloader(mode='val',batch_size=batch_size), desc=f'Epoch {epoch + 1}/{epochs}: Validation'):\n","                    outputs = self.model(image.to(device),seq.to(device))\n","                    loss = criterion(outputs.to(device), labels.to(device))\n","                    val_loss += loss.item()\n","                    _, predicted = outputs.max(1)\n","                    total += labels.size(0)\n","                    correct += predicted.eq(labels.to(device)).sum().item()\n","                val_accuracy = 100 * correct / total\n","                val_loss = val_loss / (total / batch_size)\n","            print(f'Epoch {epoch + 1}/{epochs}:')\n","            print(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}%')\n","            print(f'Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}%')\n","\n","            if val_accuracy>(max(val_acc) if len(val_acc) != 0 else 0) and savepath:\n","                filename = f'model-epoch${epoch+1}-train_loss${train_loss:.4f}-train_acc${train_accuracy:.4f}-val_loss${val_loss:.4f}-val_acc:{val_accuracy:.4f}.pth'\n","                TrainModel.clear_directory(savepath)\n","                torch.save(self.model.state_dict(), os.path.join(savepath,filename))\n","                print(f'Validation accuracy improved from {max(val_acc) if len(val_acc) != 0 else 0:.4f} to {val_accuracy:.4f}....')\n","            val_acc.append(val_accuracy)\n","\n","            if TrainModel.patience_exceed(val_acc,patience):\n","                print(f'Early stopping after {patience} epochs with no improvement in validation accuracy. Maximum validation accuracy is {np.round(np.max(val_acc),4)} on epoch {np.argmax(val_acc)+1}')\n","                break\n","\n","    def random_seed(self):\n","        seed = 7777\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed)\n","\n","    @staticmethod\n","    def patience_exceed(data,indexes):\n","        if indexes==None:\n","            return False\n","        if data[:-indexes]==[] or data[-indexes:]==[]:\n","            return False\n","        lower = max(data[:-indexes])\n","        upper = max(data[-indexes:])\n","        if upper>lower:\n","            return False\n","        else:\n","            return True\n","\n","    @staticmethod\n","    def clear_directory(path):\n","        if os.path.exists(path):\n","            for datas in os.listdir(path):\n","                files = os.path.join(path,datas)\n","                if os.path.isfile(files):\n","                    os.remove(os.path.join(path,files))\n","                elif os.path.isdir(files):\n","                    shutil.rmtree(os.path.join(path,files))\n","                elif os.path.islink(files):\n","                    os.unlink(os.path.join(path,files))\n","        else:\n","            os.makedirs(path)"]},{"cell_type":"markdown","id":"43d14616-d4a6-4b91-a486-64055e12a29a","metadata":{"id":"43d14616-d4a6-4b91-a486-64055e12a29a"},"source":["### Image Encoding"]},{"cell_type":"code","execution_count":null,"id":"1f1b5775-91f3-42a3-a86f-9b904bfb5edf","metadata":{"id":"1f1b5775-91f3-42a3-a86f-9b904bfb5edf"},"outputs":[],"source":["features = {}\n","model = encoder().to(device)\n","model.eval()\n","count = 1\n","for images in os.listdir(image_path):\n","    img = Image.open(os.path.join(image_path,images))\n","    x = convert(img).to(device)\n","    x = x.reshape(1,*x.shape)\n","    feat = model(x)\n","    # feat = x\n","    features[images] = feat.tolist()\n","    if count%100==0:\n","        print(f'{count} images are completed....')\n","    count+=1\n","with open(os.path.join(file_path,'features.pkl'),'wb') as f:\n","    pkl.dump(features,f)"]},{"cell_type":"markdown","id":"23697f43-57ed-4857-b5d2-2bdd1018a80a","metadata":{"id":"23697f43-57ed-4857-b5d2-2bdd1018a80a"},"source":["### Dataset Preprocessing"]},{"cell_type":"code","execution_count":null,"id":"6af522ff-eb63-4116-ba5c-44c4fa214b3a","metadata":{"id":"6af522ff-eb63-4116-ba5c-44c4fa214b3a"},"outputs":[],"source":["def add_tokens(dct,s):\n","    count = max(dct.values())\n","    for i in s.split():\n","        if i not in dct:\n","            count += 1\n","            dct[i] = count\n","    return dct\n","\n","def tokenizer(sentence,tokens):\n","    return [tokens[i] for i in sentence.split()]"]},{"cell_type":"code","execution_count":null,"id":"661093c5-7095-47a0-a834-5e929f0513bd","metadata":{"id":"661093c5-7095-47a0-a834-5e929f0513bd"},"outputs":[],"source":["with open(sent_path) as f:\n","    datasets = json.load(f)\n","tokens = {'<sseq>':1,'<eseq>':2}\n","captions = {}\n","tokenized_captions = {}\n","train_test_val = {}\n","for i in range(len(datasets['images'])):\n","    filename = datasets['images'][i]['filename']\n","    split = datasets['images'][i]['split']\n","    train_test_val[split] = train_test_val.get(split,[])+[filename]\n","    caption = []\n","    tokenized_caption = []\n","    for j in range(len(datasets['images'][i]['sentences'])):\n","        sentence = datasets['images'][i]['sentences'][j]\n","        tokens = add_tokens(tokens,sentence)\n","        tokenized_sentence = f'<sseq> {sentence} <eseq>'\n","        caption.append(tokenized_sentence)\n","        tokenized_caption.append(tokenizer(tokenized_sentence,tokens))\n","    captions[filename] = caption\n","    tokenized_captions[filename] = tokenized_caption\n","with open(os.path.join(file_path,'train_test_val.pkl'),'wb') as f:\n","    pkl.dump(train_test_val,f)\n","with open(os.path.join(file_path,'captions.pkl'),'wb') as f:\n","    pkl.dump(captions,f)\n","with open(os.path.join(file_path,'tokenized_captions.pkl'),'wb') as f:\n","    pkl.dump(tokenized_captions,f)\n","with open(os.path.join(file_path,'tokens.pkl'),'wb') as f:\n","    pkl.dump(tokens,f)"]},{"cell_type":"markdown","id":"dee592eb-1f9b-47f4-9a5d-d8050422b45d","metadata":{"id":"dee592eb-1f9b-47f4-9a5d-d8050422b45d"},"source":["### Batch Processing"]},{"cell_type":"code","execution_count":null,"id":"803afff1-8ec4-4836-8392-3046c5ff8cf0","metadata":{"id":"803afff1-8ec4-4836-8392-3046c5ff8cf0"},"outputs":[],"source":["class batch_preparation:\n","    def __init__(self,tokens):\n","        self.tokens = tokens\n","\n","    def __call__(self,max_len,vocab_size):\n","        input_token = self.pad_list(max_len)\n","        # output_token = self.categorical(vocab_size)\n","        output_token = self.only_position()\n","        return input_token,output_token\n","\n","    def pad_list(self,desired_len):\n","        padded = []\n","        for idx in range(1,len(self.tokens)):\n","            lst = self.tokens[:idx]\n","            padding_length = desired_len - len(lst)\n","            if padding_length > 0:\n","                padded_list = [0] * padding_length + lst\n","                padded.append(padded_list)\n","            else:\n","                return padded.append(lst)\n","        return padded\n","\n","    def categorical(self,vocab_size):\n","        cat = []\n","        for idx in range(1,len(self.tokens)):\n","            pos = self.tokens[idx]\n","            if pos < 0 or pos > vocab_size:\n","                raise ValueError(f\"Invalid position value., pos = {pos}, vocab_size = {vocab_size}\")\n","            else:\n","                cat.append([1 if i == pos else 0 for i in range(vocab_size+1)])\n","        return cat\n","\n","    def only_position(self):\n","        cat = []\n","        for idx in range(1,len(self.tokens)):\n","            pos = self.tokens[idx]\n","            cat.append(pos)\n","        return cat"]},{"cell_type":"code","execution_count":null,"id":"4e2c6dce-8fa6-412a-b088-b9f11c366964","metadata":{"id":"4e2c6dce-8fa6-412a-b088-b9f11c366964"},"outputs":[],"source":["class DataLoader:\n","    def __init__(self,impath,cappath,splitpath):\n","        with open(impath,'rb') as f:\n","            self.images = pkl.load(f)\n","        with open(cappath,'rb') as f:\n","            self.captions = pkl.load(f)\n","        with open(splitpath,'rb') as f:\n","            self.split = pkl.load(f)\n","\n","    def __call__(self,mode='train',batch_size=64,seed=7777,should_balance=True):\n","        u = self.batch_divide(mode)\n","        if seed!='no':\n","            if isinstance(seed,int):\n","                random.seed(seed)\n","            random.shuffle(u)\n","        if should_balance and len(u)%batch_size:\n","            updated_length = batch_size*math.ceil(len(u)/batch_size)\n","            diff = updated_length-len(u)\n","            u += random.sample(u,diff)\n","        for i in range(0,len(u),batch_size):\n","            temp = u[i:i+batch_size]\n","            x1 = torch.stack([[i][0][0][0] for i in temp])\n","            x2 = torch.stack([[i][0][0][1] for i in temp])\n","            y = torch.stack([[i][0][1] for i in temp])\n","            yield (x1,x2),y\n","\n","    def batch_divide(self,mode='train'):\n","        if mode.lower() not in ['train','val']:\n","            raise Exception('Only train and val mode are allowed....')\n","        select = self.split[mode]\n","        x1 = []\n","        x2 = []\n","        y = []\n","        for imgs in select:\n","            imfeat = self.images[imgs]\n","            captions = self.captions[imgs]\n","            max_len,vocab_size = self.max_length()\n","            inputs = []\n","            outputs = []\n","            for caption in captions:\n","                batch = batch_preparation(caption)\n","                input_seq,output_vector = batch(max_len,vocab_size)\n","                inputs += input_seq\n","                outputs += output_vector\n","            x1 += (imfeat*len(inputs))\n","            x2 += inputs\n","            y += outputs\n","        return [((torch.tensor(x1[i],dtype=torch.float),torch.tensor(x2[i],dtype=torch.float)),torch.tensor(y[i],dtype=torch.long)) for i in range(len(y))]\n","\n","    def max_length(self):\n","        captions = [j for i in self.captions.values() for j in i]\n","        max_len = 0\n","        vocab_size = 0\n","        for i in captions:\n","            if len(i)>max_len:\n","                max_len = len(i)\n","            if max(i)>vocab_size:\n","                vocab_size = max(i)\n","        return int(1.25*max_len),vocab_size"]},{"cell_type":"code","execution_count":null,"id":"6ebb556f-2295-4cfb-9a80-50082b8a0622","metadata":{"id":"6ebb556f-2295-4cfb-9a80-50082b8a0622"},"outputs":[],"source":["dataloader = DataLoader(os.path.join(file_path,'features.pkl'),os.path.join(file_path,'tokenized_captions.pkl'),os.path.join(file_path,'train_test_val.pkl'))"]},{"cell_type":"markdown","id":"664b4c79-ce8e-458b-99eb-8682361b7473","metadata":{"id":"664b4c79-ce8e-458b-99eb-8682361b7473"},"source":["### Model Training\n","\n","ResNet = 2048 <br>\n","VGGNet = 4096"]},{"cell_type":"code","execution_count":null,"id":"d6f1d403-f99c-47f4-b008-f31258852fed","metadata":{"id":"d6f1d403-f99c-47f4-b008-f31258852fed"},"outputs":[],"source":["model = LSTMModel(img_feat=2048,vocab_size=dataloader.max_length()[1],word_feat_dim=256).to(device)\n","trainer = TrainModel(model)\n","trainer(dataloader,savepath=model_path,patience=5)"]},{"cell_type":"markdown","id":"0a6f840d-c70e-43c1-be1a-2435ce3321b4","metadata":{"id":"0a6f840d-c70e-43c1-be1a-2435ce3321b4"},"source":["### Prediction"]},{"cell_type":"code","execution_count":null,"id":"763cfd33-c3f5-4c7f-b417-01b5c8f30b41","metadata":{"id":"763cfd33-c3f5-4c7f-b417-01b5c8f30b41"},"outputs":[],"source":["class show_datetime:\n","    def __init__(self,time):\n","        self.time = round(time)\n","    def __call__(self,string=''):\n","        minutes,seconds = int(self.time//60),self.time%60\n","        if minutes==0:\n","            if seconds>0:\n","                return f'{string}{show_datetime.formatting(\"second\",seconds)}\\033[0m'\n","            else:\n","                return f'{string}0 second\\033[0m'\n","        else:\n","            hours,minutes = int(minutes//60),int(minutes%60)\n","            if hours==0:\n","                return f'{string}{show_datetime.formatting(\"minute\",minutes)}{show_datetime.formatting(\"second\",seconds)}\\033[0m'\n","            else:\n","                days,hours = int(hours//24),int(hours%24)\n","                if days==0:\n","                    return f'{string}{show_datetime.formatting(\"hour\",hours)}{show_datetime.formatting(\"minute\",minutes)}{show_datetime.formatting(\"second\",seconds)}\\033[0m'\n","                else:\n","                    weeks,days = int(days//7),int(days%7)\n","                    if weeks==0:\n","                        return f'{string}{show_datetime.formatting(\"day\",days)}{show_datetime.formatting(\"hour\",hours)}{show_datetime.formatting(\"minute\",minutes)}{show_datetime.formatting(\"second\",seconds)}\\033[0m'\n","                    else:\n","                        raise Exception('Too big time, exceeding days....')\n","    @staticmethod\n","    def formatting(string,number):\n","        if number==0:\n","            return ''\n","        elif number==1:\n","            return f'{number} {string} '\n","        else:\n","            return f'{number} {string}s '\n","\n","class Prediction_of_Image:\n","\n","    def __init__(self, model, index, maxlen, img_feat, vocab_size, word_feat_dim,embedding_vector=None, trainable=False):\n","        self.model = LSTMModel(img_feat, vocab_size, word_feat_dim, embedding_vector=embedding_vector, trainable=trainable)\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.model = self.model.to(device)\n","        self.model.load_state_dict(torch.load(model))\n","        self.model.eval()\n","#         trainable_params = 0\n","#         non_trainable_params = 0\n","#         for param in self.model.parameters():\n","#             if param.requires_grad:\n","#                 trainable_params += param.numel()\n","#             else:\n","#                 non_trainable_params += param.numel()\n","#         print(f\"Trainable parameters: {trainable_params}\")\n","#         print(f\"Non-trainable parameters: {non_trainable_params}\")\n","        self.index = index\n","        self.rev_index = {j:i for i,j in index.items()}\n","        self.maxlen = maxlen\n","\n","    def __call__(self, photos):\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        if not isinstance(photos,torch.Tensor):\n","            photos = torch.tensor(photos,dtype=torch.float32).to(device)\n","        if len(photos.shape) != 2:\n","            photos = photos.reshape((1, -1))\n","        ind = self.index['<sseq>']\n","        seq =torch.tensor(ind,dtype=torch.long).reshape([1, -1])\n","        seq = seq.to(device)\n","        sentence = '<sseq>'\n","        with torch.no_grad():\n","            for i in range(self.maxlen-1):\n","                new_seq = F.pad(seq,(self.maxlen-seq.shape[1],0)).reshape(1,-1).to(device)\n","                pred = F.softmax(self.model(photos, new_seq).reshape(-1),dim=0)\n","                next_seq = pred.max(0).indices\n","                next_word = self.rev_index[next_seq.item()]\n","                sentence += f' {next_word}'\n","                seq = torch.cat((seq, next_seq.reshape(1, 1).to(device)), axis=1)\n","                if next_word == '<eseq>':\n","                    break\n","        sentence = ' '.join([i for i in sentence.split() if i not in ['<sseq>','<eseq>']])\n","        return sentence\n","\n","class Prediction_of_Image_Beam_Search:\n","\n","    def __init__(self, model, index, maxlen, img_feat, vocab_size, word_feat_dim,embedding_vector=None, trainable=False, beam_size=5, no_of_captions=None):\n","        self.model = LSTMModel(img_feat, vocab_size, word_feat_dim, embedding_vector=embedding_vector, trainable=trainable)\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.model = self.model.to(device)\n","        self.model.load_state_dict(torch.load(model))\n","        self.model.eval()\n","#         trainable_params = 0\n","#         non_trainable_params = 0\n","#         for param in self.model.parameters():\n","#             if param.requires_grad:\n","#                 trainable_params += param.numel()\n","#             else:\n","#                 non_trainable_params += param.numel()\n","#         print(f\"Trainable parameters: {trainable_params}\")\n","#         print(f\"Non-trainable parameters: {non_trainable_params}\")\n","        self.index = index\n","        self.rev_index = {j:i for i,j in index.items()}\n","        self.maxlen = maxlen\n","        self.beam_size = beam_size\n","        self.no_of_captions = no_of_captions if no_of_captions else beam_size\n","    def __call__(self, photos):\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        if not isinstance(photos,torch.Tensor):\n","            photos = torch.tensor(photos,dtype=torch.float32).to(device)\n","        if len(photos.shape) != 2:\n","            photos = photos.reshape((1, -1))\n","        srtind = self.index['<sseq>']\n","        endind = self.index['<eseq>']\n","        srtindt = torch.tensor(srtind,dtype=torch.long).reshape(1, 1)\n","        beams = [[srtindt, 0.0]]\n","        with torch.no_grad():\n","            for i in range(self.maxlen-1):\n","                temp = []\n","                for seq, prob in beams:\n","                    new_seq = F.pad(seq, (self.maxlen - seq.shape[1],0)).reshape(1,-1).to(device)\n","                    preds = F.softmax(self.model(photos, new_seq).reshape(-1),dim=0)\n","                    words = (preds.argsort()[-self.beam_size:]).reshape(-1)\n","                    for word in words:\n","                        seq = seq.to(device)\n","                        word = word.reshape(1,1).to(device)\n","                        fseq = torch.cat([seq, word.to(torch.long)], axis=1)\n","                        idx = word.item()\n","                        fprob = prob + torch.log2(preds[idx]).item()\n","                        temp.append([fseq, fprob])\n","                beams = sorted(temp, reverse=True, key=lambda l: l[1])[0:self.no_of_captions]\n","        sentence = max(beams,key = lambda x:x[1])[0].detach().tolist()[0]\n","        tokensd = sentence\n","        if endind in sentence:\n","            sentence = sentence[1:sentence.index(endind)]\n","        else:\n","            sentence = sentence[1:]\n","        sentence = ' '.join([self.rev_index[idx] for idx in sentence if self.rev_index[idx] not in ('<sseq>','<eseq>')])\n","        return sentence\n","\n","class Prediction_of_Image_Dataset:\n","\n","    def __init__(self, model_path, index, maxlen, image_feature_dim, vocab_size, word_feature_dim, embedding_vector=None,mode='g', arch_path=None, cap_path=None, beam_size=5, no_of_captions=None,no_of_similarities=4):\n","        if mode.lower() in ['g','greedy']:\n","            self.pred = Prediction_of_Image(model_path, index=index, maxlen=maxlen, img_feat=image_feature_dim, vocab_size=vocab_size, word_feat_dim=word_feature_dim, embedding_vector=embedding_vector, )\n","        elif mode.lower() in ['b','beam']:\n","            self.pred = Prediction_of_Image_Beam_Search(model_path, index=index, maxlen=maxlen, img_feat=image_feature_dim, vocab_size=vocab_size, word_feat_dim=word_feature_dim, embedding_vector=embedding_vector, beam_size=beam_size, no_of_captions=no_of_captions)\n","        else:\n","            raise ValueError('Only greedy (g), and beam (b) as mode are allowed.')\n","\n","    def __call__(self, images, save_path='./results/results.pkl', keep_prev=True, display_interval=100):\n","        import os\n","        import pickle as pkl\n","        import json\n","        count=0\n","        directory,filename = os.path.split(save_path)\n","        _,ext = os.path.splitext(filename)\n","        val_path = os.path.join(directory,'Values.pkl')\n","        if ext not in ['.pkl','.json']:\n","            raise TypeError('Only .pkl and .json as file extension is allowed to save captions.')\n","        if not(os.path.exists(directory)):\n","            os.makedirs(directory)\n","        if keep_prev and os.path.exists(save_path):\n","            if ext=='.pkl':\n","                with open(save_path,'rb') as f:\n","                    sentences = pkl.load(f)\n","            elif ext=='.json':\n","                with open(save_path) as f:\n","                    sentences = json.load(f)\n","        else:\n","            sentences = {}\n","        t1 = time.time()\n","        for image,feature in images.items():\n","            if image not in sentences:\n","                sentence = self.pred(feature)\n","                sentences[image] = sentence\n","            count+=1\n","            if count%display_interval==0 or count==len(images):\n","                print(f'\\033[33m{count}th image completed....\\033[0m')\n","            if count%50==0 or count==len(images):\n","                if ext=='.pkl':\n","                    with open(save_path,'wb') as f:\n","                        pkl.dump(sentences,f)\n","                elif ext=='.json':\n","                    with open(save_path,'w') as f:\n","                        json.dump(sentences,f)\n","        t2 = time.time()\n","        handle = show_datetime(t2-t1)\n","        string = '\\033[34mProcess completed in: \\033[1m'\n","        result = handle(string)\n","        print(result)"]},{"cell_type":"code","execution_count":null,"id":"3b3eb20a-542a-476c-b54c-c3ff66310a27","metadata":{"id":"3b3eb20a-542a-476c-b54c-c3ff66310a27"},"outputs":[],"source":["model_path = '/tf/DRDO/content/SYDNEY/models'\n","model = os.path.join(model_path,os.listdir(model_path)[0])\n","pred = Prediction_of_Image_Dataset(model_path=model, index=tokens, maxlen=dataloader.max_length()[0], image_feature_dim=2048, vocab_size=dataloader.max_length()[1], word_feature_dim=256,mode='g')\n","with open(os.path.join(file_path,'features.pkl'),'rb') as f:\n","    features = pkl.load(f)\n","with open(os.path.join(file_path,'train_test_val.pkl'),'rb') as f:\n","    ttvl = pkl.load(f)\n","ttvl_test = ttvl['test']\n","test_images = {img:feat for img,feat in features.items() if img in ttvl_test}\n","pred(test_images,os.path.join(pred_path,'predictions_greedy.pkl'),keep_prev=False)"]},{"cell_type":"code","execution_count":null,"id":"8e4c154d-e92b-403c-9f14-1c4c100a8509","metadata":{"id":"8e4c154d-e92b-403c-9f14-1c4c100a8509"},"outputs":[],"source":["model_path = '/tf/DRDO/content/SYDNEY/models'\n","model = os.path.join(model_path,os.listdir(model_path)[0])\n","pred = Prediction_of_Image_Dataset(model_path=model, index=tokens, maxlen=dataloader.max_length()[0], image_feature_dim=2048, vocab_size=dataloader.max_length()[1], word_feature_dim=256,mode='b')\n","with open(os.path.join(file_path,'features.pkl'),'rb') as f:\n","    features = pkl.load(f)\n","with open(os.path.join(file_path,'train_test_val.pkl'),'rb') as f:\n","    ttvl = pkl.load(f)\n","ttvl_test = ttvl['test']\n","test_images = {img:feat for img,feat in features.items() if img in ttvl_test}\n","pred(test_images,os.path.join(pred_path,'predictions_beam.pkl'),keep_prev=False)"]},{"cell_type":"markdown","id":"29fa3738-3807-46d1-8bdb-0da2c5bb94f1","metadata":{"id":"29fa3738-3807-46d1-8bdb-0da2c5bb94f1"},"source":["### Model Evaluation"]},{"cell_type":"code","execution_count":null,"id":"8329d156-191c-4203-be00-f39214ba4a97","metadata":{"id":"8329d156-191c-4203-be00-f39214ba4a97"},"outputs":[],"source":["def compute_metrics(predictions,references):\n","    if isinstance(predictions,dict):\n","        pred_keys = list(predictions)\n","        predictions = list(predictions.values())\n","    if isinstance(references,dict):\n","        refs_keys = list(references)\n","        references = list(references.values())\n","    try:\n","        if pred_keys!=refs_keys:\n","            message = \"Keys of predictions and references are either not the same or not in the same order. Unexpected results may be seen.\"\n","            warnings.warn(message)\n","    except:\n","        pass\n","    from scipy.stats import gmean\n","    rouge = evaluate.load(\"rouge\")\n","    bleu = evaluate.load(\"bleu\")\n","    meteor = evaluate.load(\"meteor\")\n","    rouge_score = round(rouge.compute(predictions=predictions, references=references, rouge_types=[\"rougeL\"])[\"rougeL\"],6)\n","    meteor_score = round(meteor.compute(predictions=predictions, references=references)[\"meteor\"],6)\n","    bleu_result = bleu.compute(predictions=predictions,references=references)\n","    brevity_penalty = bleu_result['brevity_penalty']\n","    bleu1_score = bleu_result['precisions'][0]*brevity_penalty\n","    bleu2_score = bleu_result['precisions'][1]*brevity_penalty\n","    bleu3_score = bleu_result['precisions'][2]*brevity_penalty\n","    bleu4_score = bleu_result['precisions'][3]*brevity_penalty\n","    bleu1_avg_score = round(gmean([bleu1_score]),6)\n","    bleu2_avg_score = round(gmean([bleu1_score,bleu2_score]),6)\n","    bleu3_avg_score = round(gmean([bleu1_score,bleu2_score,bleu3_score]),6)\n","    bleu4_avg_score = round(gmean([bleu1_score,bleu2_score,bleu3_score,bleu4_score]),6)\n","    return {'BLEU-1':bleu1_avg_score,'BLEU-2':bleu2_avg_score,'BLEU-3':bleu3_avg_score,'BLEU-4':bleu4_avg_score,'METEOR':meteor_score,'ROUGE-L':rouge_score}\n","\n","def filter_sentence(sentence,filter_by=['<sseq>','<eseq>']):\n","    splits = [i for i in sentence.split() if i not in filter_by]\n","    filtered_sentence = ' '.join(splits)\n","    return filtered_sentence"]},{"cell_type":"code","execution_count":null,"id":"912fab74-d7a0-494d-af95-bd54bf79eda7","metadata":{"id":"912fab74-d7a0-494d-af95-bd54bf79eda7"},"outputs":[],"source":["with open(os.path.join(file_path,'captions.pkl'),'rb') as f:\n","    captions = pkl.load(f)\n","ttvl_test = ttvl['test']\n","test_caps = {img:caps for img,caps in captions.items() if img in ttvl_test}\n","with open(os.path.join(pred_path,'predictions_beam.pkl'),'rb') as f:\n","    pred_caps = pkl.load(f)\n","keys = test_caps.keys()\n","references = [[filter_sentence(j) for j in test_caps[i]] for i in keys]\n","predictions = [filter_sentence(pred_caps[i]) for  i in keys]\n","result = compute_metrics(predictions,references)\n","print(result)"]},{"cell_type":"markdown","id":"084385c3-9726-4c74-b882-e0a4a1305f5b","metadata":{"id":"084385c3-9726-4c74-b882-e0a4a1305f5b"},"source":["### Examples"]},{"cell_type":"code","execution_count":null,"id":"c1803cb9-64d5-4424-a4f1-dba0b775e228","metadata":{"id":"c1803cb9-64d5-4424-a4f1-dba0b775e228"},"outputs":[],"source":["immodel = encoder().to(device)\n","immodel.eval();"]},{"cell_type":"code","execution_count":null,"id":"386e3ec8-6ca4-43a2-85a3-776894cbc5d3","metadata":{"id":"386e3ec8-6ca4-43a2-85a3-776894cbc5d3"},"outputs":[],"source":["img = random.choice(ttvl_test)\n","impath = os.path.join(image_path,img)\n","image = Image.open(impath)\n","image.show()\n","tensored_image = convert(image).to(device)\n","tensored_image = tensored_image.reshape(1,*tensored_image.shape)\n","feat = immodel(tensored_image)\n","model_path = '/tf/DRDO/content/SYDNEY/models'\n","model = os.path.join(model_path,os.listdir(model_path)[0])\n","\n","pred = Prediction_of_Image(model=model, index=tokens, maxlen=dataloader.max_length()[0], img_feat=2048, vocab_size=dataloader.max_length()[1], word_feat_dim=256)\n","caps = pred(feat)\n","print('\\033[1mGreedy Search: \\033[0m',caps)\n","\n","pred = Prediction_of_Image_Beam_Search(model=model, index=tokens, maxlen=dataloader.max_length()[0], img_feat=2048, vocab_size=dataloader.max_length()[1], word_feat_dim=256)\n","caps = pred(feat)\n","print('\\033[1mBeam Search: \\033[0m',caps)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0rc1"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}